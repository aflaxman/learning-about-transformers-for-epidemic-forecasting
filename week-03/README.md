# Week 3: Understanding Transformers

## Quick Links
- Open in Google Colab: https://colab.research.google.com/drive/1QYSlahOkqKKHBZN1-F_KzbHzXsM1cxD1?usp=sharing
- Related Video: https://www.youtube.com/watch?v=eMlx5fFNoYc

## Overview
This week focuses on the transformer architecture fundamentals and attention mechanisms.

- Implement scaled dot-product attention in NumPy
- Visualize attention weights for a toy sequence

More details and exercises will be added here as this week evolves.
